own

python preliminary.py --train_path data/en_ewt-ud-train.json --validation_path data/en_ewt-ud-dev.json

> Training model on dataset data/en_ewt-ud-train.json
> Finished training on 12543 samples
> All sample checks passed.
> Evaluating model on dataset data/en_ewt-ud-test.json

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.912     0.822     0.864      1782
         ADP      0.872     0.883     0.878      2030
         ADV      0.922     0.743     0.823      1147
         AUX      0.909     0.887     0.898      1509
       CCONJ      0.991     0.996     0.993       738
         DET      0.961     0.968     0.965      1898
        INTJ      0.988     0.692     0.814       120
        NOUN      0.674     0.929     0.781      4136
         NUM      0.907     0.591     0.716       541
        PART      0.665     0.992     0.796       630
        PRON      0.968     0.933     0.950      2158
       PROPN      0.902     0.531     0.668      1985
       PUNCT      0.994     0.985     0.990      3098
       SCONJ      0.632     0.650     0.641       443
         SYM      0.786     0.830     0.807       106
        VERB      0.884     0.809     0.845      2640
           X      0.333     0.015     0.028       137
    accuracy                          0.858     25098
   macro avg      0.841     0.780     0.792     25098
weighted avg      0.872     0.858     0.855     25098

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.674     0.929     0.781      4136
        PRON      0.968     0.933     0.950      2158
       PUNCT      0.994     0.985     0.990      3098
        VERB      0.884     0.809     0.845      2640
    accuracy                          0.858     25098
   macro avg      0.841     0.780     0.792     25098
weighted avg      0.872     0.858     0.855     25098




context-mean

python tagger.py train --embeddings_path data/en_ewt-ud-embeds-reduce_mean.pkl --model_path models/tagger-contextual.pkl
> Training model on dataset data/en_ewt-ud-embeds-reduce_mean.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Saving classifier to models/tagger-contextual.pkl
> Finished training on 204294 samples
Stats on Validation Set
              precision    recall  f1-score   support
         ADJ       0.94      0.92      0.93      1872
         ADP       0.96      0.98      0.97      2029
         ADV       0.91      0.91      0.91      1180
         AUX       0.97      0.98      0.98      1509
       CCONJ       0.98      0.99      0.98       779
         DET       0.99      0.98      0.99      1895
        INTJ       0.90      0.79      0.84       115
        NOUN       0.94      0.95      0.95      4196
         NUM       0.95      0.96      0.96       382
        PART       0.99      0.98      0.98       631
        PRON       0.99      0.99      0.99      2216
       PROPN       0.93      0.91      0.92      1787
       PUNCT       0.99      0.99      0.99      3075
       SCONJ       0.94      0.94      0.94       464
         SYM       0.84      0.72      0.78        79
        VERB       0.97      0.96      0.96      2757
           X       0.79      0.70      0.74       144
    accuracy                           0.96     25110
   macro avg       0.94      0.92      0.93     25110
weighted avg       0.96      0.96      0.96     25110

python tagger.py test --embeddings_path data/en_ewt-ud-embeds-reduce_mean.pkl --model_path models/tagger-contextual_mean.pkl
> Loading model from models/tagger-contextual_mean.pkl
> Evaluating model on dataset data/en_ewt-ud-embeds-reduce_mean.pkl

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.925     0.917     0.921      1782
         ADP      0.966     0.978     0.972      2028
         ADV      0.932     0.906     0.919      1147
         AUX      0.973     0.986     0.979      1508
       CCONJ      0.991     0.992     0.991       737
         DET      0.988     0.990     0.989      1898
        INTJ      0.907     0.907     0.907       118
        NOUN      0.934     0.943     0.938      4135
         NUM      0.949     0.959     0.954       540
        PART      0.981     0.989     0.985       629
        PRON      0.993     0.993     0.993      2156
       PROPN      0.912     0.890     0.901      1984
       PUNCT      0.994     0.991     0.993      3096
       SCONJ      0.955     0.950     0.952       443
         SYM      0.823     0.877     0.849       106
        VERB      0.967     0.967     0.967      2638
           X      0.687     0.672     0.679       137
    accuracy                          0.958     25082
   macro avg      0.934     0.936     0.935     25082
weighted avg      0.958     0.958     0.958     25082

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.934     0.943     0.938      4135
        PRON      0.993     0.993     0.993      2156
       PUNCT      0.994     0.991     0.993      3096
        VERB      0.967     0.967     0.967      2638
    accuracy                          0.958     25082
   macro avg      0.934     0.936     0.935     25082
weighted avg      0.958     0.958     0.958     25082




context-max
python tagger.py train --embeddings_path data/en_ewt-ud-embeds-reduce_max.pkl --model_path models/tagger-contextual_max.pkl
> Training model on dataset data/en_ewt-ud-embeds-reduce_max.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Saving classifier to models/tagger-contextual_max.pkl
> Finished training on 204294 samples
Stats on Validation Set
              precision    recall  f1-score   support
         ADJ       0.93      0.91      0.92      1872
         ADP       0.96      0.98      0.97      2029
         ADV       0.91      0.91      0.91      1180
         AUX       0.98      0.98      0.98      1509
       CCONJ       0.98      0.99      0.98       779
         DET       0.99      0.98      0.99      1895
        INTJ       0.89      0.79      0.84       115
        NOUN       0.93      0.95      0.94      4196
         NUM       0.95      0.96      0.95       382
        PART       0.99      0.98      0.98       631
        PRON       0.99      0.99      0.99      2216
       PROPN       0.91      0.90      0.91      1787
       PUNCT       0.99      0.99      0.99      3075
       SCONJ       0.94      0.93      0.93       464
         SYM       0.84      0.73      0.78        79
        VERB       0.96      0.96      0.96      2757
           X       0.76      0.67      0.71       144
    accuracy                           0.96     25110
   macro avg       0.94      0.92      0.93     25110
weighted avg       0.96      0.96      0.96     25110

python tagger.py test --embeddings_path data/en_ewt-ud-embeds-reduce_max.pkl --model_path models/tagger-contextual_max.pkl
> Loading model from models/tagger-contextual_max.pkl
> Evaluating model on dataset data/en_ewt-ud-embeds-reduce_max.pkl

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.922     0.914     0.918      1782
         ADP      0.965     0.980     0.972      2028
         ADV      0.932     0.901     0.917      1147
         AUX      0.973     0.986     0.980      1508
       CCONJ      0.992     0.992     0.992       737
         DET      0.987     0.989     0.988      1898
        INTJ      0.876     0.898     0.887       118
        NOUN      0.935     0.940     0.938      4135
         NUM      0.945     0.956     0.950       540
        PART      0.981     0.989     0.985       629
        PRON      0.994     0.991     0.993      2156
       PROPN      0.907     0.896     0.901      1984
       PUNCT      0.993     0.991     0.992      3096
       SCONJ      0.950     0.953     0.952       443
         SYM      0.857     0.849     0.853       106
        VERB      0.965     0.966     0.965      2638
           X      0.669     0.650     0.659       137
    accuracy                          0.957     25082
   macro avg      0.932     0.932     0.932     25082
weighted avg      0.957     0.957     0.957     25082

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.935     0.940     0.938      4135
        PRON      0.994     0.991     0.993      2156
       PUNCT      0.993     0.991     0.992      3096
        VERB      0.965     0.966     0.965      2638
    accuracy                          0.957     25082
   macro avg      0.932     0.932     0.932     25082
weighted avg      0.957     0.957     0.957     25082



context-sum
python tagger.py train --embeddings_path data/en_ewt-ud-embeds-reduce_sum.pkl --model_path models/tagger-contextual_sum.pkl
> Training model on dataset data/en_ewt-ud-embeds-reduce_sum.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Saving classifier to models/tagger-contextual_sum.pkl
> Finished training on 204294 samples
Stats on Validation Set
              precision    recall  f1-score   support
         ADJ       0.93      0.91      0.92      1872
         ADP       0.96      0.98      0.97      2029
         ADV       0.91      0.91      0.91      1180
         AUX       0.97      0.98      0.98      1509
       CCONJ       0.98      0.99      0.99       779
         DET       0.99      0.99      0.99      1895
        INTJ       0.90      0.79      0.84       115
        NOUN       0.93      0.95      0.94      4196
         NUM       0.96      0.96      0.96       382
        PART       0.99      0.98      0.99       631
        PRON       0.99      0.99      0.99      2216
       PROPN       0.92      0.90      0.91      1787
       PUNCT       0.99      0.99      0.99      3075
       SCONJ       0.93      0.94      0.94       464
         SYM       0.80      0.72      0.76        79
        VERB       0.97      0.96      0.96      2757
           X       0.74      0.70      0.72       144
    accuracy                           0.96     25110
   macro avg       0.93      0.92      0.93     25110
weighted avg       0.96      0.96      0.96     25110


python tagger.py test --embeddings_path data/en_ewt-ud-embeds-reduce_sum.pkl --model_path models/tagger-contextual_sum.pkl
> Loading model from models/tagger-contextual_sum.pkl
> Evaluating model on dataset data/en_ewt-ud-embeds-reduce_sum.pkl

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.918     0.915     0.916      1782
         ADP      0.966     0.979     0.972      2028
         ADV      0.928     0.904     0.916      1147
         AUX      0.973     0.985     0.979      1508
       CCONJ      0.991     0.991     0.991       737
         DET      0.987     0.989     0.988      1898
        INTJ      0.890     0.890     0.890       118
        NOUN      0.934     0.941     0.938      4135
         NUM      0.945     0.950     0.947       540
        PART      0.981     0.989     0.985       629
        PRON      0.994     0.993     0.993      2156
       PROPN      0.912     0.882     0.896      1984
       PUNCT      0.993     0.992     0.992      3096
       SCONJ      0.957     0.953     0.955       443
         SYM      0.825     0.887     0.855       106
        VERB      0.965     0.967     0.966      2638
           X      0.646     0.679     0.662       137
    accuracy                          0.957     25082
   macro avg      0.930     0.934     0.932     25082
weighted avg      0.957     0.957     0.957     25082

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.934     0.941     0.938      4135
        PRON      0.994     0.993     0.993      2156
       PUNCT      0.993     0.992     0.992      3096
        VERB      0.965     0.967     0.966      2638
    accuracy                          0.957     25082
   macro avg      0.930     0.934     0.932     25082
weighted avg      0.957     0.957     0.957     25082


context-first
python tagger.py train --embeddings_path data/en_ewt-ud-embeds-first_token.pkl --model_path models/tagger-contextual_first.pkl
> Training model on dataset data/en_ewt-ud-embeds-first_token.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Saving classifier to models/tagger-contextual_first.pkl
> Finished training on 204294 samples
Stats on Validation Set
              precision    recall  f1-score   support
         ADJ       0.92      0.91      0.91      1872
         ADP       0.96      0.98      0.97      2029
         ADV       0.91      0.90      0.90      1180
         AUX       0.97      0.98      0.97      1509
       CCONJ       0.98      0.98      0.98       779
         DET       0.98      0.98      0.98      1895
        INTJ       0.91      0.78      0.84       115
        NOUN       0.93      0.95      0.94      4196
         NUM       0.95      0.96      0.95       382
        PART       0.99      0.98      0.98       631
        PRON       0.99      0.99      0.99      2216
       PROPN       0.91      0.90      0.91      1787
       PUNCT       0.99      0.99      0.99      3075
       SCONJ       0.94      0.93      0.93       464
         SYM       0.83      0.70      0.76        79
        VERB       0.97      0.96      0.96      2757
           X       0.77      0.60      0.68       144
    accuracy                           0.96     25110
   macro avg       0.94      0.91      0.92     25110
weighted avg       0.95      0.96      0.95     25110


python tagger.py test --embeddings_path data/en_ewt-ud-embeds-first_token.pkl --model_path models/tagger-contextual_first.pkl
> Loading model from models/tagger-contextual_first.pkl
> Evaluating model on dataset data/en_ewt-ud-embeds-first_token.pkl

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.904     0.900     0.902      1782
         ADP      0.966     0.977     0.972      2028
         ADV      0.924     0.897     0.910      1147
         AUX      0.971     0.985     0.978      1508
       CCONJ      0.988     0.991     0.989       737
         DET      0.985     0.988     0.987      1898
        INTJ      0.882     0.890     0.886       118
        NOUN      0.925     0.941     0.933      4135
         NUM      0.935     0.954     0.944       540
        PART      0.978     0.989     0.983       629
        PRON      0.993     0.991     0.992      2156
       PROPN      0.908     0.876     0.892      1984
       PUNCT      0.993     0.990     0.991      3096
       SCONJ      0.957     0.948     0.952       443
         SYM      0.814     0.868     0.840       106
        VERB      0.966     0.963     0.965      2638
           X      0.689     0.613     0.649       137
    accuracy                          0.954     25082
   macro avg      0.928     0.927     0.927     25082
weighted avg      0.953     0.954     0.953     25082

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.925     0.941     0.933      4135
        PRON      0.993     0.991     0.992      2156
       PUNCT      0.993     0.990     0.991      3096
        VERB      0.966     0.963     0.965      2638
    accuracy                          0.954     25082
   macro avg      0.928     0.927     0.927     25082
weighted avg      0.953     0.954     0.953     25082



context-last

python tagger.py train --embeddings_path data/en_ewt-ud-embeds-last_token.pkl --model_path models/tagger-contextual_last.pkl
> Training model on dataset data/en_ewt-ud-embeds-last_token.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Saving classifier to models/tagger-contextual_last.pkl
> Finished training on 204294 samples
Stats on Validation Set
              precision    recall  f1-score   support
         ADJ       0.93      0.91      0.92      1872
         ADP       0.96      0.98      0.97      2029
         ADV       0.92      0.91      0.92      1180
         AUX       0.97      0.98      0.98      1509
       CCONJ       0.98      0.99      0.98       779
         DET       0.99      0.98      0.99      1895
        INTJ       0.84      0.77      0.81       115
        NOUN       0.94      0.95      0.94      4196
         NUM       0.95      0.95      0.95       382
        PART       0.99      0.98      0.98       631
        PRON       0.99      0.99      0.99      2216
       PROPN       0.92      0.90      0.91      1787
       PUNCT       0.99      0.99      0.99      3075
       SCONJ       0.94      0.94      0.94       464
         SYM       0.85      0.71      0.77        79
        VERB       0.97      0.96      0.96      2757
           X       0.67      0.68      0.67       144
    accuracy                           0.96     25110
   macro avg       0.93      0.92      0.92     25110
weighted avg       0.96      0.96      0.96     25110


python tagger.py test --embeddings_path data/en_ewt-ud-embeds-last_token.pkl --model_path models/tagger-contextual_last.pkl
> Loading model from models/tagger-contextual_last.pkl
> Evaluating model on dataset data/en_ewt-ud-embeds-last_token.pkl

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.925     0.917     0.921      1782
         ADP      0.965     0.979     0.972      2028
         ADV      0.931     0.912     0.921      1147
         AUX      0.973     0.986     0.979      1508
       CCONJ      0.993     0.993     0.993       737
         DET      0.989     0.991     0.990      1898
        INTJ      0.921     0.890     0.905       118
        NOUN      0.933     0.944     0.938      4135
         NUM      0.942     0.956     0.949       540
        PART      0.983     0.987     0.985       629
        PRON      0.993     0.992     0.993      2156
       PROPN      0.909     0.881     0.895      1984
       PUNCT      0.993     0.991     0.992      3096
       SCONJ      0.959     0.955     0.957       443
         SYM      0.839     0.887     0.862       106
        VERB      0.966     0.966     0.966      2638
           X      0.654     0.635     0.644       137
    accuracy                          0.957     25082
   macro avg      0.933     0.933     0.933     25082
weighted avg      0.957     0.957     0.957     25082

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.933     0.944     0.938      4135
        PRON      0.993     0.992     0.993      2156
       PUNCT      0.993     0.991     0.992      3096
        VERB      0.966     0.966     0.966      2638
    accuracy                          0.957     25082
   macro avg      0.933     0.933     0.933     25082
weighted avg      0.957     0.957     0.957     25082


none
python tagger.py train --embeddings_path data/en_ewt-ud-embeds-none.pkl --model_path models/tagger-none.pkl
> Training model on dataset data/en_ewt-ud-embeds-none.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Traceback (most recent call last):
  File "tagger.py", line 126, in <module>
    train(args.embeddings_path, args.baseline_strategy, args.model_path)
  File "tagger.py", line 62, in train
    classifier.fit(train_data, train_labels)
  File "/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py", line 1508, in fit
    X, y = self._validate_data(
  File "/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/base.py", line 576, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/utils/validation.py", line 973, in check_X_y
    check_consistent_length(X, y)
  File "/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/utils/validation.py", line 331, in check_consistent_length
    raise ValueError(
ValueError: Found input variables with inconsistent numbers of samples: [228373, 204294]


static
python tagger.py train --embeddings_path data/en_ewt-ud-embeds.pkl --model_path models/tagger.pkl
> Training model on dataset data/en_ewt-ud-embeds.pkl
> Training classifier with params:
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/afs/inf.ed.ac.uk/user/s21/s2173036/[anlp]/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Saving classifier to models/tagger.pkl
> Finished training on 204449 samples
Stats on Validation Set
              precision    recall  f1-score   support
         ADJ       0.85      0.82      0.84      1874
         ADP       0.85      0.89      0.87      2030
         ADV       0.87      0.66      0.75      1186
         AUX       0.87      0.89      0.88      1513
       CCONJ       0.99      0.99      0.99       779
         DET       0.95      0.98      0.96      1898
        INTJ       0.83      0.70      0.76       115
        NOUN       0.82      0.87      0.84      4197
         NUM       0.93      0.81      0.86       383
        PART       0.67      0.99      0.80       632
        PRON       0.96      0.93      0.95      2219
       PROPN       0.73      0.76      0.75      1787
       PUNCT       0.99      0.98      0.99      3076
       SCONJ       0.63      0.66      0.64       466
         SYM       0.75      0.72      0.74        80
        VERB       0.83      0.79      0.81      2760
           X       0.50      0.06      0.11       146
    accuracy                           0.86     25141
   macro avg       0.83      0.79      0.80     25141
weighted avg       0.87      0.86      0.86     25141

python tagger.py test --embeddings_path data/en_ewt-ud-embeds.pkl --model_path models/tagger.pkl
> Loading model from models/tagger.pkl
> Evaluating model on dataset data/en_ewt-ud-embeds.pkl

Full classification report:
              precision    recall  f1-score   support
         ADJ      0.855     0.817     0.836      1782
         ADP      0.867     0.883     0.875      2030
         ADV      0.864     0.709     0.779      1147
         AUX      0.900     0.885     0.893      1509
       CCONJ      0.991     0.992     0.991       738
         DET      0.955     0.975     0.965      1898
        INTJ      0.933     0.692     0.794       120
        NOUN      0.825     0.869     0.847      4136
         NUM      0.887     0.723     0.796       541
        PART      0.666     0.995     0.798       630
        PRON      0.965     0.926     0.945      2158
       PROPN      0.728     0.760     0.744      1985
       PUNCT      0.994     0.983     0.988      3098
       SCONJ      0.611     0.639     0.625       443
         SYM      0.772     0.830     0.800       106
        VERB      0.833     0.821     0.827      2640
           X      0.231     0.022     0.040       137
    accuracy                          0.867     25098
   macro avg      0.816     0.795     0.797     25098
weighted avg      0.868     0.867     0.866     25098

Classification report for top tag classes (PLEASE REPORT THIS ONE):
              precision    recall  f1-score   support
        NOUN      0.825     0.869     0.847      4136
        PRON      0.965     0.926     0.945      2158
       PUNCT      0.994     0.983     0.988      3098
        VERB      0.833     0.821     0.827      2640
    accuracy                          0.867     25098
   macro avg      0.816     0.795     0.797     25098
weighted avg      0.868     0.867     0.866     25098



