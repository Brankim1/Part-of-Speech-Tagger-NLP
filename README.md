# Part-of-Speech-Tagger-NLP
We will build a tagger to predict a part-of-speech tags from a word embedding, whether static or contextualised. It will take an English sentence as input, and output the corresponding tag for each word in the sentence.

In this coursework, we built several taggers to predict part-of-speech (POS) tags from several words embedding methods, which were the most common tagging method, static word embedding (Global Vectors for word representation (GloVe) (Pennington et al., 2014)) and contextualised word embedding (Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2018)). Word embedding is a basic procedure in Natural Language Processing (NLP) that represents words with vectors that can be easily processed by computer (Li & Yang, 2018). To compare different word embedding methods, POS taggers were implemented to probe. We focused on two questions in this coursework. Firstly, we studied the similarity and differences between the most common tagging method baseline, GloVe and BERT. Secondly, we studied the performance of model in each tag and the relationship between tag performance and the frequency of tag appearance. Finally, we concluded that the BERT had the best performance than the most common tagging method baseline and GloVe, and the frequency of tag occurrence had some moderate positive linear relationship with model performance in each tag.

In this coursework, we studied different word embedding methods via analysing the results of POS tag prediction. Two questions were investigated with quantitative and qualitative analysis. We found that BERT gave the best result followed by the GloVE and the most common tag baseline. Also, we found that the ‘CCONJ, DET, PART and PUNCT’ were identified more often correctly than other tags, and the the frequency of tag occurrence had some moderate positive linear relationship with tag performance, but not strong. However, BERT has many irrelevant words in the sentence, which might mislead or bring heavy computing. Therefore, in the future, we will continue to focus on reducing irrelevant variables to the BERT. Also, we will study the difference and similarity of word embedding methods in different languages.
